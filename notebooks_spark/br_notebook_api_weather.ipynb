{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3aa866-55f1-448a-a7d8-bb16c3cc2415",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:05.7923299Z",
       "execution_start_time": "2025-05-29T18:35:05.4412876Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "896244b0-61a7-4869-bca0-96e27748fa92",
       "queued_time": "2025-05-29T18:34:52.8413062Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": "2025-05-29T18:34:52.8423471Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Initialize Spark for Fabric environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get or create Spark session (works in both Fabric and standalone)\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    spark = SparkSession.builder.appName(\"WeatherCollection\").getOrCreate()\n",
    "\n",
    "\n",
    "API_KEY = \"your_api\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "TABLE_NAME = \"weather\"\n",
    "DELTA_TABLE_PATH = f\"Tables/{TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3c2eb4-0ee3-4245-a38e-ee884f2bf13b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:06.1006095Z",
       "execution_start_time": "2025-05-29T18:35:05.7945821Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "95e202a5-d0e8-4f46-ba4b-7ff0b5b5106e",
       "queued_time": "2025-05-29T18:34:52.8430288Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "european_cities = [\n",
    "    (\"London\", \"GB\"), (\"Paris\", \"FR\"), (\"Berlin\", \"DE\"), (\"Madrid\", \"ES\"),\n",
    "    (\"Rome\", \"IT\"), (\"Amsterdam\", \"NL\"), (\"Vienna\", \"AT\"), (\"Brussels\", \"BE\"),\n",
    "    (\"Prague\", \"CZ\"), (\"Warsaw\", \"PL\"), (\"Budapest\", \"HU\"), (\"Stockholm\", \"SE\"),\n",
    "    (\"Copenhagen\", \"DK\"), (\"Oslo\", \"NO\"), (\"Helsinki\", \"FI\"), (\"Dublin\", \"IE\"),\n",
    "    (\"Lisbon\", \"PT\"), (\"Athens\", \"GR\"), (\"Zurich\", \"CH\"), (\"Ljubljana\", \"SI\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc72432b-2249-44b7-95aa-de87be0cc011",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:06.3943267Z",
       "execution_start_time": "2025-05-29T18:35:06.1029417Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "578170be-3086-4c68-9f99-0d06a116ee25",
       "queued_time": "2025-05-29T18:34:52.8446424Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collect_today_weather_data(cities, api_key):\n",
    "    \"\"\"\n",
    "    Collect weather data for current day only\n",
    "    \"\"\"\n",
    "    weather_data = []\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    print(f\"ğŸŒ¤ï¸ Collecting data for {today}\")\n",
    "    \n",
    "    for city, country_code in cities:\n",
    "        print(f\"   ğŸ“ {city}, {country_code}\")\n",
    "        \n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                weather_record = {\n",
    "                    'country_code': country_code,\n",
    "                    'city_name': city,\n",
    "                    'date': today,\n",
    "                    'temperature_avg': float(data['main']['temp']),\n",
    "                    'temperature_max': float(data['main']['temp_max']),\n",
    "                    'temperature_min': float(data['main']['temp_min']),\n",
    "                    'humidity': int(data['main']['humidity']),\n",
    "                    'pressure': int(data['main']['pressure']),\n",
    "                    'weather_condition': data['weather'][0]['main'],\n",
    "                    'weather_description': data['weather'][0]['description'],\n",
    "                    'wind_speed': float(data.get('wind', {}).get('speed', 0)),\n",
    "                    'cloudiness': int(data.get('clouds', {}).get('all', 0)),\n",
    "                    'created_at': current_timestamp\n",
    "                }\n",
    "                \n",
    "                weather_data.append(weather_record)\n",
    "                \n",
    "            time.sleep(0.1)  # Rate limiting for free API\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error for {city}: {e}\")\n",
    "    \n",
    "    print(f\"âœ… Collected {len(weather_data)} records\")\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30e3bbb-625c-42e1-bf9e-d726355e6311",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:06.6700975Z",
       "execution_start_time": "2025-05-29T18:35:06.3963361Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "45ecbc89-4a8a-4ea3-86ef-e4f2072a6ea6",
       "queued_time": "2025-05-29T18:34:52.8461427Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_existing_data(spark, table_name):\n",
    "    \"\"\"\n",
    "    Load existing data from delta table or return None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to read from delta table\n",
    "        existing_df = spark.read.format(\"delta\").table(table_name)\n",
    "        record_count = existing_df.count()\n",
    "        date_range = existing_df.select(\n",
    "            min(col(\"date\")).alias(\"min_date\"),\n",
    "            max(col(\"date\")).alias(\"max_date\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"ğŸ“‚ Existing delta table found: {record_count} records\")\n",
    "        print(f\"ğŸ“… Current period: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ“‚ No existing delta table found: {e}\")\n",
    "        print(\"ğŸ“‚ Creating new delta table.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa792067-c3eb-4149-a0f0-9a180f2be1f6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:06.9905308Z",
       "execution_start_time": "2025-05-29T18:35:06.6720945Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e3bf0ddb-43f3-4492-bd9e-c91831440deb",
       "queued_time": "2025-05-29T18:34:52.8476178Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_today_already_exists(existing_df, today_date):\n",
    "    \"\"\"\n",
    "    Check if data for today already exists\n",
    "    \"\"\"\n",
    "    if existing_df is None:\n",
    "        return False\n",
    "    \n",
    "    today_count = existing_df.filter(col(\"date\") == today_date).count()\n",
    "    return today_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c154742-30f7-4ebe-91ef-709e36a7d949",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:07.2807235Z",
       "execution_start_time": "2025-05-29T18:35:06.9928979Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b6c28d9a-a494-45c7-85e1-ef10c91c109d",
       "queued_time": "2025-05-29T18:34:52.849118Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_weather_schema():\n",
    "    \"\"\"\n",
    "    Define schema for weather data\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"country_code\", StringType(), True),\n",
    "        StructField(\"city_name\", StringType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"temperature_avg\", DoubleType(), True),\n",
    "        StructField(\"temperature_max\", DoubleType(), True),\n",
    "        StructField(\"temperature_min\", DoubleType(), True),\n",
    "        StructField(\"humidity\", IntegerType(), True),\n",
    "        StructField(\"pressure\", IntegerType(), True),\n",
    "        StructField(\"weather_condition\", StringType(), True),\n",
    "        StructField(\"weather_description\", StringType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"cloudiness\", IntegerType(), True),\n",
    "        StructField(\"created_at\", StringType(), True)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3fcc25-e5de-422b-9f2d-00608f5efb34",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:07.5752453Z",
       "execution_start_time": "2025-05-29T18:35:07.2828477Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "fab52b2f-63db-44f6-a516-77f63ce83824",
       "queued_time": "2025-05-29T18:34:52.8508521Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_derived_columns(df):\n",
    "    \"\"\"\n",
    "    Add derived columns for analysis\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "             .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "             .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "             .withColumn(\"day_of_week\", dayofweek(col(\"date\"))) \\\n",
    "             .withColumn(\"week_of_year\", weekofyear(col(\"date\"))) \\\n",
    "             .withColumn(\n",
    "                 \"season\",\n",
    "                 when(month(col(\"date\")).isin([12, 1, 2]), \"Winter\")\n",
    "                 .when(month(col(\"date\")).isin([3, 4, 5]), \"Spring\")\n",
    "                 .when(month(col(\"date\")).isin([6, 7, 8]), \"Summer\")\n",
    "                 .otherwise(\"Autumn\")\n",
    "             ) \\\n",
    "             .withColumn(\n",
    "                 \"temperature_category\",\n",
    "                 when(col(\"temperature_avg\") < 0, \"Freezing\")\n",
    "                 .when(col(\"temperature_avg\") < 10, \"Cold\")\n",
    "                 .when(col(\"temperature_avg\") < 20, \"Mild\")\n",
    "                 .when(col(\"temperature_avg\") < 30, \"Warm\")\n",
    "                 .otherwise(\"Hot\")\n",
    "             ) \\\n",
    "             .withColumn(\n",
    "                 \"is_weekend\",\n",
    "                 when(col(\"day_of_week\").isin([1, 7]), True).otherwise(False)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c639493-754a-4be1-aa09-bb6a1d5ea387",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:07.8733515Z",
       "execution_start_time": "2025-05-29T18:35:07.5773579Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f95bb9c8-6967-4f5b-a301-728d0c4ed372",
       "queued_time": "2025-05-29T18:34:52.8524702Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_and_save_data(spark, existing_df, new_df, table_name):\n",
    "    \"\"\"\n",
    "    Merge existing data with new data and save to delta table\n",
    "    \"\"\"\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Add derived columns to new data\n",
    "    new_df_enhanced = add_derived_columns(new_df)\n",
    "    \n",
    "    if existing_df is None:\n",
    "        # First run - create new delta table\n",
    "        print(\"ğŸ“ First execution - creating new delta table\")\n",
    "        final_df = new_df_enhanced\n",
    "        \n",
    "        # Create delta table\n",
    "        final_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(table_name)\n",
    "                \n",
    "    else:\n",
    "        # Existing table - use merge operation\n",
    "        print(\"ğŸ”„ Merging with existing delta table\")\n",
    "        \n",
    "        # Remove today's data if exists (to overwrite)\n",
    "        existing_df_filtered = existing_df.filter(col(\"date\") != today)\n",
    "        \n",
    "        # Union existing data with new data\n",
    "        combined_df = existing_df_filtered.union(new_df_enhanced)\n",
    "        \n",
    "        # Overwrite the table\n",
    "        combined_df.write \\\n",
    "                   .format(\"delta\") \\\n",
    "                   .mode(\"overwrite\") \\\n",
    "                   .option(\"mergeSchema\", \"true\") \\\n",
    "                   .saveAsTable(table_name)\n",
    "        \n",
    "        final_df = combined_df\n",
    "    \n",
    "    # Sort by date and city for final result\n",
    "    final_df = final_df.orderBy(\"date\", \"city_name\")\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c11214-119d-4cf8-b965-1347c046f973",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:08.1965426Z",
       "execution_start_time": "2025-05-29T18:35:07.8760412Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b5f16c91-58cd-4cb8-b8ca-cb50a16e641c",
       "queued_time": "2025-05-29T18:34:52.8541166Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_statistics(df):\n",
    "    \"\"\"\n",
    "    Display dataset statistics\n",
    "    \"\"\"\n",
    "    total_records = df.count()\n",
    "    unique_dates = df.select(\"date\").distinct().count()\n",
    "    unique_cities = df.select(\"city_name\").distinct().count()\n",
    "    date_range = df.select(\n",
    "        min(col(\"date\")).alias(\"min_date\"),\n",
    "        max(col(\"date\")).alias(\"max_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… SUCCESS! Data updated\")\n",
    "    print(f\"ğŸ“Š Total records: {total_records}\")\n",
    "    print(f\"ğŸ“… Complete period: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "    print(f\"ğŸ™ï¸ Cities: {unique_cities}\")\n",
    "    print(f\"ğŸ“ˆ Unique dates: {unique_dates}\")\n",
    "    \n",
    "    # Recent week summary\n",
    "    recent_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    recent_df = df.filter(col(\"date\") >= recent_date)\n",
    "    \n",
    "    if recent_df.count() > 0:\n",
    "        print(f\"\\nğŸ“‹ Last week summary:\")\n",
    "        recent_summary = recent_df.groupBy(\"city_name\") \\\n",
    "                                 .agg(\n",
    "                                     avg(\"temperature_avg\").alias(\"avg_temp\"),\n",
    "                                     count(\"date\").alias(\"day_count\")\n",
    "                                 ) \\\n",
    "                                 .orderBy(\"city_name\")\n",
    "        recent_summary.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50939331-0862-434e-b901-8f90681a1586",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:08.5608408Z",
       "execution_start_time": "2025-05-29T18:35:08.1988048Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8013b755-c4da-4c60-af72-cd8103c3afe7",
       "queued_time": "2025-05-29T18:34:52.8556863Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function - daily execution\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Starting incremental weather data collection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # 1. Load existing data\n",
    "    existing_df = load_existing_data(spark, TABLE_NAME)\n",
    "    \n",
    "    # 2. Check if today's data already exists\n",
    "    if check_today_already_exists(existing_df, today):\n",
    "        print(f\"âš ï¸ Data for {today} already exists. Overwriting...\")\n",
    "    \n",
    "    # 3. Collect today's data\n",
    "    new_data = collect_today_weather_data(european_cities, API_KEY)\n",
    "    \n",
    "    if new_data:\n",
    "        # Create DataFrame with schema\n",
    "        schema = create_weather_schema()\n",
    "        new_df = spark.createDataFrame(new_data, schema)\n",
    "        \n",
    "        # 4. Merge and save to delta table\n",
    "        final_df = merge_and_save_data(spark, existing_df, new_df, TABLE_NAME)\n",
    "        \n",
    "        # 5. Show statistics\n",
    "        show_statistics(final_df)\n",
    "        print(\"ğŸ¯ Delta table created successfully!\")\n",
    "        print(f\"ğŸ“Š Table: {TABLE_NAME}\")\n",
    "        print(f\"ğŸ’¾ Format: Delta Lake\")\n",
    "        print(f\"ğŸ—‚ï¸ Schema: Managed table in lakehouse\")\n",
    "        print(f\"\\nğŸ” Data Quality Checks:\")\n",
    "        null_check = final_df.select([\n",
    "            count(when(col(c).isNull(), c)).alias(c) \n",
    "            for c in [\"temperature_avg\", \"humidity\", \"pressure\"]\n",
    "        ])\n",
    "        null_check.show()\n",
    "        \n",
    "        # Temperature range check\n",
    "        temp_stats = final_df.select(\n",
    "            min(\"temperature_avg\").alias(\"min_temp\"),\n",
    "            max(\"temperature_avg\").alias(\"max_temp\"),\n",
    "            avg(\"temperature_avg\").alias(\"avg_temp\")\n",
    "        )\n",
    "        temp_stats.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No data collected today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da77a066-c9b7-44f0-9165-3d3e2e717dc9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-29T18:35:58.0442366Z",
       "execution_start_time": "2025-05-29T18:35:08.5628251Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "9af9de07-0458-49cb-905f-410ea4ec3a23",
       "queued_time": "2025-05-29T18:34:52.857252Z",
       "session_id": "c43030d5-d044-403b-b647-e532c9c9f8d9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, c43030d5-d044-403b-b647-e532c9c9f8d9, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting incremental weather data collection\n",
      "============================================================\n",
      "ğŸ“‚ Existing delta table found: 20 records\n",
      "ğŸ“… Current period: 2025-05-28 to 2025-05-28\n",
      "ğŸŒ¤ï¸ Collecting data for 2025-05-29\n",
      "   ğŸ“ London, GB\n",
      "   ğŸ“ Paris, FR\n",
      "   ğŸ“ Berlin, DE\n",
      "   ğŸ“ Madrid, ES\n",
      "   ğŸ“ Rome, IT\n",
      "   ğŸ“ Amsterdam, NL\n",
      "   ğŸ“ Vienna, AT\n",
      "   ğŸ“ Brussels, BE\n",
      "   ğŸ“ Prague, CZ\n",
      "   ğŸ“ Warsaw, PL\n",
      "   ğŸ“ Budapest, HU\n",
      "   ğŸ“ Stockholm, SE\n",
      "   ğŸ“ Copenhagen, DK\n",
      "   ğŸ“ Oslo, NO\n",
      "   ğŸ“ Helsinki, FI\n",
      "   ğŸ“ Dublin, IE\n",
      "   ğŸ“ Lisbon, PT\n",
      "   ğŸ“ Athens, GR\n",
      "   ğŸ“ Zurich, CH\n",
      "   ğŸ“ Ljubljana, SI\n",
      "âœ… Collected 20 records\n",
      "ğŸ”„ Merging with existing delta table\n",
      "============================================================\n",
      "âœ… SUCCESS! Data updated\n",
      "ğŸ“Š Total records: 40\n",
      "ğŸ“… Complete period: 2025-05-28 to 2025-05-29\n",
      "ğŸ™ï¸ Cities: 20\n",
      "ğŸ“ˆ Unique dates: 2\n",
      "\n",
      "ğŸ“‹ Last week summary:\n",
      "+---------+------------------+---------+\n",
      "|city_name|          avg_temp|day_count|\n",
      "+---------+------------------+---------+\n",
      "|Amsterdam|14.450000000000001|        2|\n",
      "|   Athens|21.229999999999997|        2|\n",
      "|   Berlin|             15.72|        2|\n",
      "| Brussels|            16.165|        2|\n",
      "| Budapest|             17.03|        2|\n",
      "+---------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "ğŸ¯ Delta table created successfully!\n",
      "ğŸ“Š Table: weather\n",
      "ğŸ’¾ Format: Delta Lake\n",
      "ğŸ—‚ï¸ Schema: Managed table in lakehouse\n",
      "\n",
      "ğŸ” Data Quality Checks:\n",
      "+---------------+--------+--------+\n",
      "|temperature_avg|humidity|pressure|\n",
      "+---------------+--------+--------+\n",
      "|              0|       0|       0|\n",
      "+---------------+--------+--------+\n",
      "\n",
      "+--------+--------+------------------+\n",
      "|min_temp|max_temp|          avg_temp|\n",
      "+--------+--------+------------------+\n",
      "|    8.64|   32.31|17.436749999999996|\n",
      "+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in main execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd431ee2-0e7b-4ea5-bf5f-ce91b5a0692b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "709c9ee2-9b9c-4730-9332-ce20ffbb5f2e",
    "default_lakehouse_name": "lh1",
    "default_lakehouse_workspace_id": "42bace73-a356-473e-8a2b-7f3cdd601b8f",
    "known_lakehouses": [
     {
      "id": "709c9ee2-9b9c-4730-9332-ce20ffbb5f2e"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
